

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Overview of AMReX GPU Strategy &mdash; amrex 19.02-dev documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Visualization" href="Visualization_Chapter.html" />
    <link rel="prev" title="GPU" href="GPU_Chapter.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> amrex
          

          
          </a>

          
            
            
              <div class="version">
                19.02-dev
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">AMReX Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="GettingStarted_Chapter.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="BuildingAMReX_Chapter.html">Building AMReX</a></li>
<li class="toctree-l1"><a class="reference internal" href="Basics_Chapter.html">Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tutorials_Chapter.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="AmrCore_Chapter.html">AmrCore Source Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="AmrLevel_Chapter.html">Amr Source Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="AsyncIter_Chapter.html">Asynchronous Iterators (AmrTask)</a></li>
<li class="toctree-l1"><a class="reference internal" href="IO_Chapter.html">I/O (Plotfile, Checkpoint)</a></li>
<li class="toctree-l1"><a class="reference internal" href="LinearSolvers_Chapter.html">Linear Solvers</a></li>
<li class="toctree-l1"><a class="reference internal" href="Particle_Chapter.html">Particles</a></li>
<li class="toctree-l1"><a class="reference internal" href="Fortran_Chapter.html">Fortran Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="EB_Chapter.html">Embedded Boundaries</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="GPU_Chapter.html">GPU</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Overview of AMReX GPU Strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-gpu-support">Building GPU Support</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#building-with-gnu-make">Building with GNU Make</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-with-cmake">Building with CMake</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gpu-namespace-and-macros">Gpu Namespace and Macros</a></li>
<li class="toctree-l2"><a class="reference internal" href="#memory-allocation">Memory Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpu-safe-classes-and-functions">GPU Safe Classes and Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gpuarray">GpuArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="#managedvector">ManagedVector</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cuda-s-thrust-vectors">CUDA’s Thrust Vectors</a></li>
<li class="toctree-l3"><a class="reference internal" href="#amrex-min-and-amrex-max">amrex::min and amrex::max</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multifab-reductions">MultiFab Reductions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#box-intvect-and-indextype">Box, IntVect and IndexType</a></li>
<li class="toctree-l3"><a class="reference internal" href="#geometry">Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="#basefab-farraybox-and-iarraybox">BaseFab, FArrayBox and IArrayBox</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#kernel-launch">Kernel Launch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#launching-a-c-function">Launching a C++ function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#launching-a-fortran-function">Launching a FORTRAN function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#offloading-work-using-openacc-pragmas">Offloading work using OpenACC pragmas</a></li>
<li class="toctree-l3"><a class="reference internal" href="#launching-an-generic-loop">Launching an generic loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kernel-launch-details">Kernel launch details</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#an-example-of-migrating-to-gpu">An Example of Migrating to GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="#assertions-error-checking-and-synchronization">Assertions, Error Checking and Synchronization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#particle-support">Particle Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="#profiling-with-gpus">Profiling with GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performance-tips">Performance Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="#limitations">Limitations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Visualization_Chapter.html">Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="AMReX_Profiling_Tools_Chapter.html">AMReX-based Profiling Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="External_Profiling_Tools_Chapter.html">External Profiling Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="External_Frameworks_Chapter.html">External Frameworks</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">amrex</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="GPU_Chapter.html">GPU</a> &raquo;</li>
        
      <li>Overview of AMReX GPU Strategy</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/GPU.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="overview-of-amrex-gpu-strategy">
<span id="sec-gpu-overview"></span><h1>Overview of AMReX GPU Strategy<a class="headerlink" href="#overview-of-amrex-gpu-strategy" title="Permalink to this headline">¶</a></h1>
<p>AMReX’s GPU strategy focuses on providing performant GPU support
with minimal changes and maximum flexibility.  This allows
application teams to get running on GPUs quickly while allowing
long term perfomance tuning and programming model selection.  AMReX
uses CUDA for GPUs, but application teams can use CUDA, CUDA
Fortran, OpenACC or OpenMP in their individual codes.</p>
<p>When running AMReX on a CPU system, the parallelization strategy is a
combination of MPI and OpenMP using tiling, as detailed in
<a class="reference internal" href="Basics.html#sec-basics-mfiter-tiling"><span class="std std-ref">MFIter with Tiling</span></a>. However, cache blocking and data
locality are not the primary considerations when writing optimal code
on GPUs.  Instead, efficient use of the GPU’s resources is the primary
concern.  Improving resource efficiency allows a larger percentage of
GPU threads to work simultaneously, increasing effective parallelism and
decreasing the time to solution.</p>
<p>The GPU strategy that has shown to best match AMReX’s mesh and particle
implementations is <code class="docutils literal notranslate"><span class="pre">MPI+OpenMP</span></code> for CPUs and <code class="docutils literal notranslate"><span class="pre">MPI+CUDA</span></code> for GPUs:
<code class="docutils literal notranslate"><span class="pre">(MPI+OpenMP)</span> <span class="pre">+</span> <span class="pre">(MPI+CUDA)</span></code>. Presented here is an overview of important
features of AMReX’s GPU strategy. Additional information that is required
for creating GPU applications is detailed throughout the rest of this
chapter:</p>
<ul class="simple">
<li>To ensure data consistency, each MPI rank offloads its work to a single
GPU.  This ensures each kernel works on a GPU where the associated data
is consistently located and minimizes communication between GPUs.
<code class="docutils literal notranslate"><span class="pre">(MPI</span> <span class="pre">ranks</span> <span class="pre">==</span> <span class="pre">Number</span> <span class="pre">of</span> <span class="pre">GPUs)</span></code></li>
<li>Calculations that can be offloaded efficiently to GPUs use CUDA threads
to parallelize over a valid box at a time.  This is done by using a lot
of CUDA threads that only work on a few cells each. This work
distribution is illustrated in <a class="reference internal" href="#fig-gpu-threads"><span class="std std-numref">Table 9</span></a>.
(Note: OpenMP is currently incompatible with AMReX builds using CUDA.
This feature is under development, although most applications will
have no need for this feature when initially converting to GPUs.)</li>
</ul>
<span id="fig-gpu-threads"></span><table border="1" class="docutils" id="id1">
<caption><span class="caption-number">Table 9 </span><span class="caption-text">Comparison of OpenMP and CUDA work distribution. Pictures provided by Mike Zingale and the CASTRO team.</span><a class="headerlink" href="#id1" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/gpu_2.png"><img alt="a" src="_images/gpu_2.png" style="width: 100%;" /></a></td>
<td><a class="reference internal" href="_images/gpu_3.png"><img alt="b" src="_images/gpu_3.png" style="width: 100%;" /></a></td>
</tr>
<tr class="row-even"><td><div class="first last line-block">
<div class="line">OpenMP tiled box.</div>
<div class="line">OpenMP threads break down the valid box
into two large boxes (blue and orange).
The lo and hi of one tiled box are marked.</div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">CUDA threaded box.</div>
<div class="line">Each CUDA thread works on a few cells of the
valid box. This example uses one cell per
thread, each thread using a box with lo = hi.</div>
</div>
</td>
</tr>
</tbody>
</table>
<ul>
<li><p class="first">C++ macros and CUDA extended lambdas are used to provide performance
portability while making the code as understandable as possible to
science-focused code teams.</p>
</li>
<li><p class="first">AMReX’s data movement plan is to initialize mesh and particle data
structures, move them to the GPU and leave them in GPU memory space
as much as possible.  This strategy lends itself to AMReX
applications readily; the mesh and particle data should be able
to stay on the GPU except when performing redistribution and I/O
operations.  Application teams should strive to this data
management strategy as much as possible to achieve good GPU performance.</p>
</li>
<li><p class="first">AMReX utilizes CUDA managed memory to automatically handle memory
movement for mesh and particle data.  Simple data structures, such
as <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">IntVect</span></span></code>s can be passed by value and temporaries, such as
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code>es, have specialized AMReX classes to handle the
data movement for the user.  Tests have shown CUDA managed memory
to be efficient and reliable, especially when applications remove
any unnecessary data accesses.</p>
</li>
<li><p class="first">AMReX’s GPU strategy is focused on launching GPU kernels inside
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loops.  By performing GPU work within <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code>
loops, GPU work is isolated to independent data sets on simple AMReX data
objects, providing consistency and safety that matches AMReX’s coding
methodology.</p>
</li>
<li><p class="first">AMReX further parallelizes GPU applications by utilizing CUDA streams.
A CUDA stream is a list of GPU kernel launches that is ran on the GPU
sequentially.  Kernel launches placed in different CUDA streams can be run
simultaneously on the GPU, given enough computing resources are available.
AMReX places each iteration of <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loops on separate streams,
allowing each independent iteration to be run simultaneously and maximize
available GPU resources.</p>
<p>The AMReX implementation of CUDA streams is illustrated in <a class="reference internal" href="#fig-gpu-streams"><span class="std std-numref">Fig. 13</span></a>.
The CPU runs the first iteration of the MFIter loop (blue), which contains three
GPU kernels.  The kernels begin immediately in GPU Stream 1 and run in the same
order they were added. The second (red) and third (green) iterations are similarly
launched in Streams 2 and 3. The fourth (orange) and fifth (purple) iterations
require more GPU resources than remain, so they have to wait until resources are
freed before beginning. Meanwhile, after all the loop iterations are launched, the
CPU reaches a synchronize in the MFIter’s destructor and waits for all GPU launches
to complete before continuing.</p>
</li>
</ul>
<div class="figure" id="id2">
<span id="fig-gpu-streams"></span><img alt="_images/Streams.png" src="_images/Streams.png" />
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">Timeline illustration of GPU streams. Illustrates the case of an
MFIter loop of five iterations with three GPU kernels each being
ran with three GPU streams.</span></p>
</div>
</div>
<div class="section" id="building-gpu-support">
<span id="sec-gpu-build"></span><h1>Building GPU Support<a class="headerlink" href="#building-gpu-support" title="Permalink to this headline">¶</a></h1>
<div class="section" id="building-with-gnu-make">
<h2>Building with GNU Make<a class="headerlink" href="#building-with-gnu-make" title="Permalink to this headline">¶</a></h2>
<p>To build AMReX with GPU support, add <code class="docutils literal notranslate"><span class="pre">USE_CUDA=TRUE</span></code> to the
<code class="docutils literal notranslate"><span class="pre">GNUmakefile</span></code> or as a command line argument.  AMReX does not
require OpenACC or CUDA Fortran, but application codes can use them
if they are supported by the compiler.  For OpenACC support, add
<code class="docutils literal notranslate"><span class="pre">USE_ACC=TRUE</span></code>.  PGI, Cray and GNU compilers support OpenACC.  Thus,
for OpenACC, you must use <code class="docutils literal notranslate"><span class="pre">COMP=pgi</span></code>, <code class="docutils literal notranslate"><span class="pre">COMP=cray</span></code> or <code class="docutils literal notranslate"><span class="pre">COMP=gnu</span></code>.
Only IBM and PGI support CUDA Fortran, which is also built when
<code class="docutils literal notranslate"><span class="pre">USE_CUDA=TRUE</span></code>.  OpenMP is currently not supported with CUDA.</p>
<p>Compiling AMReX with CUDA requires compiling the code through NVIDIA’s
CUDA compiler driver in addition to the standard compiler.  This driver
is called <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> and it requires a host compiler to work through.
The default host compiler for NVCC is GCC even if <code class="docutils literal notranslate"><span class="pre">COMP</span></code> is set to
a different compiler.  One can change this by setting <code class="docutils literal notranslate"><span class="pre">NVCC_HOST_COMP</span></code>.
For example, <code class="docutils literal notranslate"><span class="pre">COMP=pgi</span></code> alone will compile C/C++ codes with NVCC/GCC
and Fortran codes with PGI, and link with PGI.  Using <code class="docutils literal notranslate"><span class="pre">COMP=pgi</span></code> and
<code class="docutils literal notranslate"><span class="pre">NVCC_HOST_COMP=pgi</span></code> will compile C/C++ codes with PGI and NVCC/PGI.</p>
<p>You can use <code class="docutils literal notranslate"><span class="pre">Tutorials/Basic/HelloWorld_C</span></code> to test your programming
environment.  Building with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">make COMP=gnu USE_CUDA=TRUE</span>
</pre></div>
</div>
<p>should produce an executable named <code class="docutils literal notranslate"><span class="pre">main3d.gnu.DEBUG.CUDA.ex</span></code>.  You
can run it and that will generate results like:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span> ./main3d.gnu.DEBUG.CUDA.ex
<span class="go">CUDA initialized with 1 GPU</span>
<span class="go">AMReX (18.12-95-gf265b537f479-dirty) initialized</span>
<span class="go">Hello world from AMReX version 18.12-95-gf265b537f479-dirty</span>
<span class="go">[The         Arena] space (kilobyte): 8192</span>
<span class="go">[The  Device Arena] space (kilobyte): 8192</span>
<span class="go">[The Managed Arena] space (kilobyte): 8192</span>
<span class="go">[The  Pinned Arena] space (kilobyte): 8192</span>
<span class="go">AMReX (18.12-95-gf265b537f479-dirty) finalized</span>
</pre></div>
</div>
</div>
<div class="section" id="building-with-cmake">
<h2>Building with CMake<a class="headerlink" href="#building-with-cmake" title="Permalink to this headline">¶</a></h2>
<p>CMake is currently unavailable when building with GPUs.</p>
</div>
</div>
<div class="section" id="gpu-namespace-and-macros">
<span id="sec-gpu-namespace"></span><h1>Gpu Namespace and Macros<a class="headerlink" href="#gpu-namespace-and-macros" title="Permalink to this headline">¶</a></h1>
<p>Most GPU related classes and functions are in <code class="docutils literal notranslate"><span class="pre">namespace</span> <span class="pre">Gpu</span></code>,
which is inside <code class="docutils literal notranslate"><span class="pre">namespace</span> <span class="pre">amrex</span></code>. For example, the GPU configuration
class <code class="docutils literal notranslate"><span class="pre">Device</span></code> can be referenced to at <code class="docutils literal notranslate"><span class="pre">amrex::Gpu::Device</span></code>. Other
important objects in the Gpu namespace include <code class="docutils literal notranslate"><span class="pre">Gpu::AsyncFab</span></code>.</p>
<p>For portability, AMReX defines some macros for CUDA function qualifiers
and they should be preferred to allow execution with <code class="docutils literal notranslate"><span class="pre">USE_CUDA=FALSE</span></code>.
These include:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#define AMREX_GPU_HOST        __host__</span>
<span class="cp">#define AMREX_GPU_DEVICE      __device__</span>
<span class="cp">#define AMREX_GPU_GLOBAL      __global__</span>
<span class="cp">#define AMREX_GPU_HOST_DEVICE __host__ __device__</span>
</pre></div>
</div>
<p>Note that when AMReX is not built with CUDA, these macros expand to
empty space.</p>
<p>When AMReX is compiled with <code class="docutils literal notranslate"><span class="pre">USE_CUDA=TRUE</span></code>, the preprocessor
macros <code class="docutils literal notranslate"><span class="pre">AMREX_USE_CUDA</span></code> and <code class="docutils literal notranslate"><span class="pre">AMREX_USE_GPU</span></code> are defined for
conditional programming.  For PGI and IBM compilers,
<code class="docutils literal notranslate"><span class="pre">AMREX_USE_CUDA_FORTRAN</span></code> is also defined, as well as
<code class="docutils literal notranslate"><span class="pre">-DAMREX_CUDA_FORT_GLOBAL='attributes(global)'</span></code>,
<code class="docutils literal notranslate"><span class="pre">-DAMREX_CUDA_FORT_DEVICE='attributes(device)'</span></code>, and
<code class="docutils literal notranslate"><span class="pre">-DAMREX_CUDA_FORT_HOST='attributes(host)'</span></code> so that CUDA Fortran
functions can be properly labelled.  When AMReX is compiled with
<code class="docutils literal notranslate"><span class="pre">USE_ACC=TRUE</span></code>, <code class="docutils literal notranslate"><span class="pre">AMREX_USE_ACC</span></code> is defined.</p>
<p>In addition to AMReX’s preprocessor macros, CUDA provides the
<code class="docutils literal notranslate"><span class="pre">__CUDA_ARCH__</span></code> macro which is only defined when in device code.
<code class="docutils literal notranslate"><span class="pre">__CUDA_ARCH__</span></code> should be used when a <code class="docutils literal notranslate"><span class="pre">__host__</span> <span class="pre">__device__</span></code>
function requires separate code for the CPU and GPU implementations.</p>
</div>
<div class="section" id="memory-allocation">
<span id="sec-gpu-memory"></span><h1>Memory Allocation<a class="headerlink" href="#memory-allocation" title="Permalink to this headline">¶</a></h1>
<p>To provide portability and improve memory allocation performance,
AMReX provides a number of memory pools.  When compiled without
CUDA, all <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Arena</span></span></code>s implement standard <code class="code cpp c++ docutils literal notranslate"><span class="keyword"><span class="pre">new</span></span></code> and
<code class="code cpp c++ docutils literal notranslate"><span class="keyword"><span class="pre">delete</span></span></code> operators. Without CUDA, the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Arena</span></span></code>s each
allocate with a specific type of GPU memory:</p>
<span id="tab-gpu-arena"></span><table border="1" class="docutils" id="id3">
<caption><span class="caption-number">Table 10 </span><span class="caption-text">Memory Arenas</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="54%" />
<col width="46%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Arena</th>
<th class="head">Memory Type</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>The_Arena()</td>
<td>unified memory</td>
</tr>
<tr class="row-odd"><td>The_Device_Arena()</td>
<td>device memory</td>
</tr>
<tr class="row-even"><td>The_Managed_Arena()</td>
<td>unified memory</td>
</tr>
<tr class="row-odd"><td>The_Pinned_Arena()</td>
<td>pinned memory</td>
</tr>
</tbody>
</table>
<p>The Arena object returned by these calls provides access
to two functions:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="o">*</span> <span class="nf">alloc</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">sz</span><span class="p">);</span>
<span class="kt">void</span> <span class="nf">free</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span> <span class="n">p</span><span class="p">);</span>
</pre></div>
</div>
<p><code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">The_Arena</span></span><span class="punctuation"><span class="pre">()</span></span></code> is used for memory allocation of data in
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">BaseFab</span></span></code>.  Therefore the data in a <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span></code> is placed in
unified memory and is accessible from both CPU host and GPU device.
This allows application codes to develop their GPU capability
gradually.  <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">The_Managed_Arena</span></span><span class="punctuation"><span class="pre">()</span></span></code> is a separate pool of
unified memory, that is distinguished from <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">The_Arena</span></span><span class="punctuation"><span class="pre">()</span></span></code> for
performance reasons.  If you want to print out the current memory usage
of the Arenas, you can call <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">Arena</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">PrintUsage</span></span><span class="punctuation"><span class="pre">()</span></span></code>.</p>
</div>
<div class="section" id="gpu-safe-classes-and-functions">
<span id="sec-gpu-classes"></span><h1>GPU Safe Classes and Functions<a class="headerlink" href="#gpu-safe-classes-and-functions" title="Permalink to this headline">¶</a></h1>
<p>AMReX GPU work takes place inside of MFIter and particle loops.
Therefore, there are two ways classes and functions have been modified
to interact with the GPU:</p>
<p>1. A number of functions used within these loops are labelled using
<code class="docutils literal notranslate"><span class="pre">AMREX_GPU_HOST_DEVICE</span></code> and can be called on the device. This includes member
functions, such as <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">IntVect</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">type</span></span><span class="punctuation"><span class="pre">()</span></span></code>, as well as non-member functions,
such as <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">min</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">max</span></span></code>. In specialized cases,
classes are labeled such that the object can be constructed, destructed
and its functions can be implemented on the device, including <code class="docutils literal notranslate"><span class="pre">IntVect</span></code>.</p>
<p>2. Functions that contain MFIter or particle loops have been rewritten
to contain device launches. For example, the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FillBoundary</span></span></code>
function cannot be called from device code, but calling it from
CPU will launch GPU kernels if AMReX is compiled with GPU support.</p>
<p>Most functions and objects have not been given a device version due to
CUDA restrictions and to ensure applications maintain an internally consistent
GPU strategy.</p>
<p>In this section, we discuss some examples of AMReX device classes and functions
that are important for programming GPUs.</p>
<div class="section" id="gpuarray">
<h2>GpuArray<a class="headerlink" href="#gpuarray" title="Permalink to this headline">¶</a></h2>
<p><code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">std</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">array</span></span></code> is used throughout AMReX, however its functions are not defined
in device code. GpuArray is AMReX’s built-in alternative. It uses a C array that
can be passed to the device by value and has device functions for the <code class="code cpp c++ docutils literal notranslate"><span class="punctuation"><span class="pre">[]</span></span></code>
operator, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">size</span></span><span class="punctuation"><span class="pre">()</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">data</span></span><span class="punctuation"><span class="pre">()</span></span></code> that returns a pointer to the C array.
GpuArray can be used whenever a fixed size array of data needs to be passed to the GPU.</p>
<p>Additional functions have been created to return GpuArray instead of <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">std</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">array</span></span></code>,
including <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">GeometryData</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">CellSizeArray</span></span><span class="punctuation"><span class="pre">()</span></span></code>, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">GeometryData</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">InvCellSizeArray</span></span><span class="punctuation"><span class="pre">()</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Box</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">length3d</span></span><span class="punctuation"><span class="pre">()</span></span></code>.</p>
</div>
<div class="section" id="managedvector">
<h2>ManagedVector<a class="headerlink" href="#managedvector" title="Permalink to this headline">¶</a></h2>
<p>AMReX also provides a dynamic memory allocation object for GPU managed memory:
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ManagedVector</span></span></code>.  This class behaves identically to an
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">Vector</span></span></code>, (see <a class="reference internal" href="Basics.html#sec-basics-vecandarr"><span class="std std-ref">Vector and Array</span></a>), except the vector’s
allocator has been changed to allocate and deallocate its data in CUDA
managed memory whenever <code class="docutils literal notranslate"><span class="pre">USE_CUDA=TRUE</span></code>.</p>
<p>While the data is managed and available on GPUs, the member functions of
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ManagedVector</span></span></code> are not. To use the data on the GPU, it is
necessary to pass the underlying data pointer to the GPU. The managed data
pointer can be accessed using the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">data</span></span><span class="punctuation"><span class="pre">()</span></span></code> member function.</p>
<p>Be aware: resizing of dynamically allocated memory on the GPU is unsupported.
All resizing of the vector should be done on the CPU, in a manner that avoids
race conditions with concurrent GPU kernels.</p>
</div>
<div class="section" id="cuda-s-thrust-vectors">
<h2>CUDA’s Thrust Vectors<a class="headerlink" href="#cuda-s-thrust-vectors" title="Permalink to this headline">¶</a></h2>
<p>CUDA’s Thrust library can also be used to manage dynamically sized data sets.
However, if Thrust is used directly in AMReX code, it will be unable to compile
for cases when <code class="docutils literal notranslate"><span class="pre">USE_CUDA=FALSE</span></code>.  To alleviate this issue,
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">thrust</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">host_vector</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">thrust</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">device_vector</span></span></code> have been wrapped
into the AMReX classes <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">HostVector</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">DeviceVector</span></span></code>,
When <code class="docutils literal notranslate"><span class="pre">USE_CUDA=FALSE</span></code>, these classes revert to AMReX’s Vector class. When
<code class="docutils literal notranslate"><span class="pre">USE_CUDA=TRUE</span></code>, these classes become the corresponding Thrust vector.</p>
<p>Just like with Thrust vectors, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">HostVector</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">DeviceVector</span></span></code> cannot
be directly used on the device. For convenience, the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">dataPtr</span></span><span class="punctuation"><span class="pre">()</span></span></code> member
function has been altered to implement <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">thrust</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">raw_pointer_cast</span></span></code> and
return the raw data pointer which can be used to access the vector’s underlying
data on the GPU.</p>
<p>It has proven useful to have a version of Thrust’s <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">device_vector</span></span></code>
that uses CUDA managed memory. This is provided by <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ManagedDeviceVector</span></span></code>.</p>
<p><code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">thrust</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">copy</span></span></code> is also commonly used in AMReX applications. It can be
implemented portably using <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">thrust_copy</span></span></code>.</p>
</div>
<div class="section" id="amrex-min-and-amrex-max">
<h2>amrex::min and amrex::max<a class="headerlink" href="#amrex-min-and-amrex-max" title="Permalink to this headline">¶</a></h2>
<p>GPU versions of <code class="docutils literal notranslate"><span class="pre">std::min</span></code> and <code class="docutils literal notranslate"><span class="pre">std::max</span></code> are not provided in CUDA.
So, AMReX provides a templated <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">min</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">max</span></span></code> with host and
device versions to allow functionality on GPUs. Invoke the explicitly
namespaced <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">min</span></span><span class="punctuation"><span class="pre">(</span></span><span class="name"><span class="pre">A</span></span><span class="punctuation"><span class="pre">,</span></span> <span class="name"><span class="pre">B</span></span><span class="punctuation"><span class="pre">)</span></span></code> or <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">max</span></span><span class="punctuation"><span class="pre">(</span></span><span class="name"><span class="pre">x</span></span><span class="punctuation"><span class="pre">,</span></span> <span class="name"><span class="pre">y</span></span><span class="punctuation"><span class="pre">)</span></span></code> to use the
GPU safe implementations.</p>
</div>
<div class="section" id="multifab-reductions">
<h2>MultiFab Reductions<a class="headerlink" href="#multifab-reductions" title="Permalink to this headline">¶</a></h2>
<p>AMReX provides functions for performing standard reduction operations on
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFabs</span></span></code>, including <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">sum</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">max</span></span></code>.
When <code class="docutils literal notranslate"><span class="pre">USE_CUDA=TRUE</span></code>, these functions automatically implement the
corresponding reductions on GPUs in a highly efficient manner.</p>
<p>Function templates <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ReduceSum</span></span></code>, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ReduceMin</span></span></code> and
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ReduceMax</span></span></code> can be used to implement user-defined reduction
functions over <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span></code>s. These same templates are implemented
in the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span></code> functions, so they can be used as a reference to
build a custom reduction. For example, the <code class="code cpp c++ docutils literal notranslate"><span class="name label"><span class="pre">MultiFab</span></span><span class="punctuation"><span class="pre">:</span></span><span class="name"><span class="pre">Dot</span></span></code>
implementation is reproduced here:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">Real</span> <span class="n">sm</span> <span class="o">=</span> <span class="n">amrex</span><span class="o">::</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">nghost</span><span class="p">,</span>
<span class="p">[</span><span class="o">=</span><span class="p">]</span> <span class="n">AMREX_GPU_HOST_DEVICE</span> <span class="p">(</span><span class="n">Box</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">bx</span><span class="p">,</span> <span class="n">FArrayBox</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">xfab</span><span class="p">,</span> <span class="n">FArrayBox</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">yfab</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Real</span>
<span class="p">{</span>
    <span class="k">return</span> <span class="n">xfab</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">bx</span><span class="p">,</span><span class="n">xcomp</span><span class="p">,</span><span class="n">yfab</span><span class="p">,</span><span class="n">bx</span><span class="p">,</span><span class="n">ycomp</span><span class="p">,</span><span class="n">numcomp</span><span class="p">);</span>
<span class="p">});</span>

<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">local</span><span class="p">)</span> <span class="n">ParallelAllReduce</span><span class="o">::</span><span class="n">Sum</span><span class="p">(</span><span class="n">sm</span><span class="p">,</span> <span class="n">ParallelContext</span><span class="o">::</span><span class="n">CommunicatorSub</span><span class="p">());</span>

<span class="k">return</span> <span class="n">sm</span><span class="p">;</span>
</pre></div>
</div>
<p><code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ReduceSum</span></span></code> takes two <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span></code>s, <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> and
returns the sum of the value returned from the given lambda function.
In this case, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">BaseFab</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">dot</span></span></code> is returned, yielding a sum of the
dot product of each local pair of <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">BaseFab</span></span></code>s. Finally,
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">ParallelAllReduce</span></span></code> is used to sum the dot products across all
MPI ranks and return the total dot product of the two
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span></code>s.</p>
<p>To implement a different reduction, replace the code block inside the
lambda function with the operation that should be applied, being sure
to return the value to be summed, minimized, or maximized.  The reduction
templates have a few different interfaces to accomodate a variety of
reductions.  The <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ReduceSum</span></span></code> reduction template has varieties
that take either one, two or three :<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span></code>s.
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ReduceMin</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ReduceMax</span></span></code> can take either one
or two.</p>
</div>
<div class="section" id="box-intvect-and-indextype">
<h2>Box, IntVect and IndexType<a class="headerlink" href="#box-intvect-and-indextype" title="Permalink to this headline">¶</a></h2>
<p>In AMReX, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Box</span></span></code>, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">IntVect</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">IndexType</span></span></code>
are classes for representing indices.  These classes and most of
their member functions, including constructors and destructors,
have both host and device versions.  They can be used in
device code.</p>
</div>
<div class="section" id="geometry">
<h2>Geometry<a class="headerlink" href="#geometry" title="Permalink to this headline">¶</a></h2>
<p>AMReX’s <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Geometry</span></span></code> class is not a GPU safe class.  However, we often need
to use geometric information such as cell size and physical coordinates
in GPU kernels.  To utilize <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Geometry</span></span></code> on the GPUs, the data is copied
into a GPU safe class that can be passed by value to GPU kernels. This class
is called <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">GeometryData</span></span></code>, which is created by calling
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Geometry</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">data</span></span><span class="punctuation"><span class="pre">()</span></span></code>. The accessor functions of <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">GeometryData</span></span></code> are
identical to <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Geometry</span></span></code>.</p>
<p>One limitation of this strategy is that <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Geometry</span></span></code> cannot be changed
on the device. <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">GeometryData</span></span></code> holds a disposable copy of the data that
does not synchronize with <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Geometry</span></span></code> after use. Therefore, only change
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Geometry</span></span></code> on the CPU and outside of MFIter loops with GPU kernels to
avoid race conditions.</p>
</div>
<div class="section" id="basefab-farraybox-and-iarraybox">
<h2>BaseFab, FArrayBox and IArrayBox<a class="headerlink" href="#basefab-farraybox-and-iarraybox" title="Permalink to this headline">¶</a></h2>
<p><code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">BaseFab</span></span><span class="operator"><span class="pre">&lt;</span></span><span class="name"><span class="pre">T</span></span><span class="operator"><span class="pre">&gt;</span></span></code>, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">IArrayBox</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code>
have some GPU support.  They cannot be constructed in device code, but
a pointer to them can be passed to GPU kernels from CPU code.  Many
of their member functions can be used in device code as long as they
have been allocated in device memory. Some of the device member
functions include <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">view</span></span></code>, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">dataPtr</span></span></code>, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">box</span></span></code>,
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">nComp</span></span></code>, and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">setVal</span></span></code>.</p>
<p>All <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">BaseFab</span></span><span class="operator"><span class="pre">&lt;</span></span><span class="name"><span class="pre">T</span></span><span class="operator"><span class="pre">&gt;</span></span></code> objects in <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FabArray</span></span><span class="operator"><span class="pre">&lt;</span></span><span class="name"><span class="pre">FAB</span></span><span class="operator"><span class="pre">&gt;</span></span></code> are allocated in
unified memory, including <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">IArrayBox</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code>, which are
derived from <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">BaseFab</span></span></code>. A <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">BaseFab</span></span><span class="operator"><span class="pre">&lt;</span></span><span class="name"><span class="pre">T</span></span><span class="operator"><span class="pre">&gt;</span></span></code> object created
on the stack in CPU code cannot be used in GPU device code, because
the object is in CPU memory.  However, a <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">BaseFab</span></span></code> created with
<code class="code cpp c++ docutils literal notranslate"><span class="keyword"><span class="pre">new</span></span></code> on the heap is GPU safe, because <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">BaseFab</span></span></code> has its own
overloaded <code class="code cpp c++ docutils literal notranslate"><span class="keyword"><span class="pre">operator</span></span> <span class="keyword"><span class="pre">new</span></span></code> that allocates memory from
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">The_Arena</span></span><span class="punctuation"><span class="pre">()</span></span></code>, a managed memory arena.  For example,</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// We are in CPU code</span>

<span class="n">FArrayBox</span> <span class="nf">cpu_fab</span><span class="p">(</span><span class="n">box</span><span class="p">,</span><span class="n">ncomp</span><span class="p">);</span>
<span class="c1">// FArrayBox* p_cpu_fab = &amp;(cpu_fab) cannot be used in GPU device code!</span>

<span class="n">FArrayBox</span><span class="o">*</span> <span class="n">p_gpu_fab</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FArrayBox</span><span class="p">(</span><span class="n">box</span><span class="p">,</span><span class="n">ncomp</span><span class="p">);</span>
<span class="c1">// FArrayBox* p_gpu_fab can be used in GPU device code.</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="kernel-launch">
<span id="sec-gpu-launch"></span><h1>Kernel Launch<a class="headerlink" href="#kernel-launch" title="Permalink to this headline">¶</a></h1>
<p>In this section, how to offload work to the GPU will be demonstrated.
In the CUDA C++ and Fortran cases, the kernel is launched with a C++
template global function in AMReX (hidden in an AMReX launch macro),
whereas for the OpenACC example, it is done with pragmas in Fortran.
More details on the examples can be found in the source codes at
<code class="docutils literal notranslate"><span class="pre">Tutorials/GPU/Launch/</span></code>.  We also refer the readers to Chapter
<a class="reference internal" href="Basics_Chapter.html#chap-basics"><span class="std std-ref">Basics</span></a> for information about basic AMReX classes.</p>
<p>It is also recommended to write your primary floating point operation
kernels in C++ using AMReX’s <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FabView</span></span></code> object syntax. It converts
the 1D array into a simple to understand 3D loop structure, similar
to Fortran while maintaining performance. The details can be found in
<a class="reference internal" href="Basics.html#sec-basics-cppkernel"><span class="std std-ref">C++ Kernel</span></a>.</p>
<div class="section" id="launching-a-c-function">
<h2>Launching a C++ function<a class="headerlink" href="#launching-a-c-function" title="Permalink to this headline">¶</a></h2>
<p>The part launching a CUDA C++ kernel is shown below.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">MFIter</span> <span class="n">mfi</span><span class="p">(</span><span class="n">mf</span><span class="p">,</span><span class="n">TilingIfNotGPU</span><span class="p">());</span> <span class="n">mfi</span><span class="p">.</span><span class="n">isValid</span><span class="p">();</span> <span class="o">++</span><span class="n">mfi</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="n">Box</span><span class="o">&amp;</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">mfi</span><span class="p">.</span><span class="n">tilebox</span><span class="p">();</span>
    <span class="n">FArrayBox</span><span class="o">*</span> <span class="n">fab</span> <span class="o">=</span> <span class="n">mf</span><span class="p">.</span><span class="n">fabPtr</span><span class="p">(</span><span class="n">mfi</span><span class="p">);</span>
    <span class="n">AMREX_LAUNCH_DEVICE_LAMBDA</span> <span class="p">(</span> <span class="n">bx</span><span class="p">,</span> <span class="n">tbx</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="n">plusone_cudacpp</span><span class="p">(</span><span class="n">tbx</span><span class="p">,</span> <span class="o">*</span><span class="n">fab</span><span class="p">);</span>
    <span class="p">});</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The code above works whether it is compiled for GPUs or CPUs.
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">TilingIfNotGPU</span></span><span class="punctuation"><span class="pre">()</span></span></code> returns <code class="docutils literal notranslate"><span class="pre">false</span></code> in the GPU case to
turn off tiling so that GPU kernels have more compute work to do.
When tiling is off, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">tilebox</span></span><span class="punctuation"><span class="pre">()</span></span></code> returns the valid box of the
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code> for that iteration.  The <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">fabPtr</span></span></code>
function takes <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> and returns a managed pointer that can be
captured by an extended C++ lambda function the user defined in the
<code class="docutils literal notranslate"><span class="pre">AMREX_LAUNCH_DEVICE_LAMBDA</span></code> launch macro.</p>
<p>The launch macro takes three arguments.  The first argument is a
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Box</span></span></code> specifying the whole region of the kernel.  The second
argument is the desired name of a <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Box</span></span></code> variable that
specifies the subregion each thread works on.  This subregion
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Box</span></span></code>  is defined with the specificed name inside the macro.
The third  argument denotes the code block of the lambda function
that will be ran in this launch. In this example, a GPU device
function, <code class="docutils literal notranslate"><span class="pre">plusone_cudacpp</span></code>, is called and is passed the captured
variable <code class="docutils literal notranslate"><span class="pre">fab</span></code>.  In CUDA, an extended lambda function can only
capture by value, not reference.  That’s why a pointer must be created
to the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code>.</p>
</div>
<div class="section" id="launching-a-fortran-function">
<h2>Launching a FORTRAN function<a class="headerlink" href="#launching-a-fortran-function" title="Permalink to this headline">¶</a></h2>
<p>We can also call CUDA Fortran device functions in the code block for
the launch macro like below.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">MFIter</span> <span class="n">mfi</span><span class="p">(</span><span class="n">mf</span><span class="p">,</span><span class="n">TilingIfNotGPU</span><span class="p">());</span> <span class="n">mfi</span><span class="p">.</span><span class="n">isValid</span><span class="p">();</span> <span class="o">++</span><span class="n">mfi</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="n">Box</span><span class="o">&amp;</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">mfi</span><span class="p">.</span><span class="n">tilebox</span><span class="p">();</span>
    <span class="n">FArrayBox</span><span class="o">*</span> <span class="n">fab</span> <span class="o">=</span> <span class="n">mf</span><span class="p">.</span><span class="n">fabPtr</span><span class="p">(</span><span class="n">mfi</span><span class="p">);</span>
    <span class="n">AMREX_LAUNCH_DEVICE_LAMBDA</span> <span class="p">(</span> <span class="n">bx</span><span class="p">,</span> <span class="n">tbx</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="n">plusone_cudafort</span><span class="p">(</span><span class="n">BL_TO_FORTRAN_BOX</span><span class="p">(</span><span class="n">tbx</span><span class="p">),</span>
                         <span class="n">BL_TO_FORTRAN_ANYD</span><span class="p">(</span><span class="o">*</span><span class="n">fab</span><span class="p">));</span>
    <span class="p">});</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Because <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Box</span></span></code> and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code> are C++ classes not understood by
Fortran, we use some helper macros to pass them as Fortran data types
(see <a class="reference internal" href="Basics.html#sec-basics-fortran"><span class="std std-ref">Fortran, C and C++ Kernels</span></a>).</p>
</div>
<div class="section" id="offloading-work-using-openacc-pragmas">
<h2>Offloading work using OpenACC pragmas<a class="headerlink" href="#offloading-work-using-openacc-pragmas" title="Permalink to this headline">¶</a></h2>
<p>The tutorial at <code class="docutils literal notranslate"><span class="pre">Tutorials/GPU/Launch</span></code> also shows an example of
using OpenACC in Fortran.  We call a Fortran function and in that
function we use OpenACC to offload work to GPU.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">MFIter</span> <span class="n">mfi</span><span class="p">(</span><span class="n">mf</span><span class="p">,</span><span class="n">TilingIfNotGPU</span><span class="p">());</span> <span class="n">mfi</span><span class="p">.</span><span class="n">isValid</span><span class="p">();</span> <span class="o">++</span><span class="n">mfi</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="n">Box</span><span class="o">&amp;</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">mfi</span><span class="p">.</span><span class="n">tilebox</span><span class="p">();</span>
    <span class="n">FArrayBox</span><span class="o">&amp;</span> <span class="n">fab</span> <span class="o">=</span> <span class="n">mf</span><span class="p">[</span><span class="n">mfi</span><span class="p">];</span>
    <span class="n">plusone_acc</span><span class="p">(</span><span class="n">BL_TO_FORTRAN_BOX</span><span class="p">(</span><span class="n">tbx</span><span class="p">),</span>
                <span class="n">BL_TO_FORTRAN_ANYD</span><span class="p">(</span><span class="n">fab</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Note that here <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span><span class="operator"><span class="pre">::</span></span><span class="keyword"><span class="pre">operator</span></span><span class="punctuation"><span class="pre">[]</span></span></code> is used to get a reference
to <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code> rather than <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">fabPtr</span></span></code> to get
a pointer, as was done in the CUDA examples.  Using the reference is
optimal for performance when passing pointers to kernels is not
required, which includes CPU code sections and OpenACC implementations.</p>
<p>Function <code class="docutils literal notranslate"><span class="pre">plusone_acc</span></code> is a CPU host function.  The reference from
<code class="code cpp c++ docutils literal notranslate"><span class="keyword"><span class="pre">operator</span></span><span class="punctuation"><span class="pre">[]</span></span></code> is a reference to a <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code> in host
memory even though the data pointer inside the object points to
unified memory.  This managed data pointer is retrieved with
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">fabPtr</span></span></code>.  <code class="docutils literal notranslate"><span class="pre">BL_TO_FORTRAN_ANYD</span></code> expands to the individual
components of the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code>, including the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Box</span></span></code> defining
its indicies, the number of components and the data pointer itself.
By passing the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code> via its required components,
unnecessary data movement is minimized.</p>
<p>The corresponding OpenACC labelled loop in <code class="docutils literal notranslate"><span class="pre">plusone_acc</span></code> is:</p>
<div class="highlight-fortran notranslate"><div class="highlight"><pre><span></span><span class="c">!dat = pointer to fab&#39;s managed data</span>

<span class="c">!$acc kernels deviceptr(dat)</span>
<span class="k">do       </span><span class="n">k</span> <span class="o">=</span> <span class="n">lo</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">hi</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
   <span class="k">do    </span><span class="n">j</span> <span class="o">=</span> <span class="n">lo</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">hi</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
      <span class="k">do </span><span class="n">i</span> <span class="o">=</span> <span class="n">lo</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">hi</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
         <span class="n">dat</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">k</span><span class="p">)</span> <span class="o">=</span> <span class="n">dat</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">k</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.0_amrex_real</span>
      <span class="k">end do</span>
<span class="k">   end do</span>
<span class="k">end do</span>
<span class="c">!$acc end kernels</span>
</pre></div>
</div>
<p>Since the data pointer passed to <code class="docutils literal notranslate"><span class="pre">plusone_acc</span></code> points to
unified memory, OpenACC is told the data is available on the device
by using the <code class="docutils literal notranslate"><span class="pre">deviceptr</span></code> construct.</p>
</div>
<div class="section" id="launching-an-generic-loop">
<h2>Launching an generic loop<a class="headerlink" href="#launching-an-generic-loop" title="Permalink to this headline">¶</a></h2>
<p>The previous examples showed how the <code class="docutils literal notranslate"><span class="pre">AMREX_LAUNCH_DEVICE_LAMBDA</span></code>
macro can be used to launch threads that work across cells in a
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code> inside of an <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loop. However, the macro
is also capable of launching for a specified number of iterations
that will be split across GPU threads. For example, launching over
the number of elements in a vector is an example given in
<code class="docutils literal notranslate"><span class="pre">Tutorials/GPU/Launch</span></code> and is reproduced here:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>
    <span class="n">amrex</span><span class="o">::</span><span class="n">Gpu</span><span class="o">::</span><span class="n">ManagedVector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">const</span> <span class="k">auto</span> <span class="n">data</span> <span class="o">=</span> <span class="n">ones</span><span class="p">.</span><span class="n">dataPtr</span><span class="p">();</span>
    <span class="n">AMREX_LAUNCH_DEVICE_LAMBDA</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">iter</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="n">data</span><span class="p">[</span><span class="n">iter</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">iter</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
    <span class="p">});</span>

    <span class="n">Gpu</span><span class="o">::</span><span class="n">Device</span><span class="o">::</span><span class="n">synchronize</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">ManagedVector</span></span></code> is created that can be worked on from
both the CPU and GPU.  A copy of the underlying data pointer is
created, so it can be captured and passed into the lambda function.
The <code class="docutils literal notranslate"><span class="pre">AMREX_LAUNCH_DEVICE_LAMBDA</span></code> launch macro is used and again
it takes three arguments: the size of the vector as a <code class="code cpp c++ docutils literal notranslate"><span class="keyword type"><span class="pre">long</span></span></code>,
a name for the loop iterator and a lambda function to perform on
each iteration.</p>
<p>This form of the macro can be used to create any standard, singly
incremented loop on a GPU, not just over a vector.  This macro
can also work on any contiguous subsets of a data set by passing
a pointer that points to the beginning of the subset and the size
of the subset.</p>
<p>Users must be aware that there is no automatic device synchronize,
as these launches do not have to occur inside of an <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code>
loop.  Add <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">Device</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">synchronize</span></span><span class="punctuation"><span class="pre">()</span></span></code> where necessary to
ensure GPU and CPU resources do not alter the same data
simultaneously.  Lack of automatic GPU stream incremenation is
another consequence of being outside an <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loop. Users
must be aware that if this generic launch is inside a
non-<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loop all launches will be placed in the default
stream, leading to a synchronous implementation of the launches.</p>
</div>
<div class="section" id="kernel-launch-details">
<h2>Kernel launch details<a class="headerlink" href="#kernel-launch-details" title="Permalink to this headline">¶</a></h2>
<p>CUDA kernel calls are asynchronous and they return before the kernel
is finished on the GPU. So <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> finishes its iterations on
the CPU before the GPU finishes its work.  To guarantee consistency,
there is an implicit CUDA device synchronization (a CUDA barrier) in
the destructor of <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code>.  This ensures that all GPU work
inside of an <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loop will complete before code outside of
the loop is executed.</p>
<p>CUDA supports multiple streams and kernels. Kernels launched in the
same stream are executed sequentially, but different streams of kernel
launches may be run in parallel.  For each iteration of <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code>,
AMReX uses a different CUDA stream (up to 16 streams in total) for the
kernels in that iteration.  This allows each iteration of an <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code>
loop to run indepenently and maximize the use of GPU resources while
writting clean, readable <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loops.</p>
<p>Launching kernels with the <code class="docutils literal notranslate"><span class="pre">AMREX_LAUNCH_DEVICE_LAMBDA</span></code> uses the CUDA
extended lamdba feature.  Extended lambdas have some restrictions the user
must understand.  For example, the function enclosing the extended
lamdba must not have private or protected access within its parent class,
otherwise the code will not compile.  This can be fixed by changing the
access of the enclosing function to public.</p>
<p>Another pitfall that <em>must</em> be considered: if the
extended lambda accesses a member of the enclosing class, the lambda
function actually captures <code class="code cpp c++ docutils literal notranslate"><span class="keyword"><span class="pre">this</span></span></code> pointer by value and accesses
variable via <code class="code cpp c++ docutils literal notranslate"><span class="keyword"><span class="pre">this</span></span><span class="operator"><span class="pre">-&gt;</span></span></code>.  If the object is not accessible on GPU,
the code will not work as intended.  For example,</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyClass</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
    <span class="n">Box</span> <span class="n">bx</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">m</span><span class="p">;</span>                           <span class="c1">// Unmanaged integer created on the host.</span>
    <span class="kt">void</span> <span class="nf">f</span> <span class="p">()</span> <span class="p">{</span>
        <span class="n">AMREX_LAUNCH_DEVICE_LAMBDA</span> <span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">tbx</span><span class="p">,</span>
        <span class="p">{</span>
            <span class="n">printf</span><span class="p">(</span><span class="s">&quot;m = %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span>   <span class="c1">// Failed attempt to use m on the GPU.</span>
        <span class="p">});</span>
    <span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The function <code class="docutils literal notranslate"><span class="pre">f</span></code> in the code above will not work unless <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MyClass</span></span></code>
object is in unified memory.  If it is undesirable to put the class into
unified memory, a local copy of the information can be created for the
lambda to capture. For example:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyClass</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
    <span class="n">Box</span> <span class="n">bx</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">m</span><span class="p">;</span>
    <span class="kt">void</span> <span class="nf">f</span> <span class="p">()</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">local_m</span> <span class="o">=</span> <span class="n">m</span><span class="p">;</span>                  <span class="c1">// Local temporary copy of m.</span>
        <span class="n">AMREX_LAUNCH_DEVICE_LAMBDA</span> <span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">tbx</span><span class="p">,</span>
        <span class="p">{</span>
            <span class="n">printf</span><span class="p">(</span><span class="s">&quot;m = %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">local_m</span><span class="p">);</span>  <span class="c1">// Lambda captures local_m by value.</span>
        <span class="p">});</span>
    <span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Finally, AMReX’s expected OpenMP strategy for GPUs is to utilize OpenMP in
CPU regions to maintain multi-threaded parallelism on work that cannot be
offloaded efficiently, while using CUDA independently in GPU regions:
(MPI+OpenMP)+(MPI+CUDA).  This means OpenMP pragmas need to be maintained
when <code class="docutils literal notranslate"><span class="pre">USE_CUDA=FALSE</span></code> and turned off in locations CUDA is implemented
when <code class="docutils literal notranslate"><span class="pre">USE_CUDA=TRUE</span></code>.</p>
<p>This can currently be implemented in preparation for an OpenMP strategy and
users are highly encouraged to do so now. This prevents having to track
down and label the appropriate OpenMP regions in the future and
clearly labels for readers that OpenMP and GPUs are not being used at the
same time.  OpenMP pragmas can be turned off using the conditional pragma
and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">notInLaunchRegion</span></span><span class="punctuation"><span class="pre">()</span></span></code>, as shown below:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#ifdef _OPENMP</span>
<span class="cp">#pragma omp parallel if (Gpu::notInLaunchRegion())</span>
<span class="cp">#endif</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="an-example-of-migrating-to-gpu">
<span id="sec-gpu-example"></span><h1>An Example of Migrating to GPU<a class="headerlink" href="#an-example-of-migrating-to-gpu" title="Permalink to this headline">¶</a></h1>
<p>The nature of GPU programming poses difficulties for a number
of common AMReX patterns, such as the one below:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Given MultiFab uin and uout</span>
<span class="n">FArrayBox</span> <span class="n">q</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="n">MFIter</span> <span class="n">mfi</span><span class="p">(</span><span class="n">uin</span><span class="p">);</span> <span class="n">mfi</span><span class="p">.</span><span class="n">isValid</span><span class="p">();</span> <span class="o">++</span><span class="n">mfi</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="n">Box</span><span class="o">&amp;</span> <span class="n">vbx</span> <span class="o">=</span> <span class="n">mfi</span><span class="p">.</span><span class="n">validbox</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">Box</span><span class="o">&amp;</span> <span class="n">gbx</span> <span class="o">=</span> <span class="n">amrex</span><span class="o">::</span><span class="n">grow</span><span class="p">(</span><span class="n">vbx</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
    <span class="n">q</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">gbx</span><span class="p">);</span>

    <span class="c1">// Do some work with uin[mfi] as input and q as output.</span>
    <span class="c1">// The output region is gbx;</span>
    <span class="n">f1</span><span class="p">(</span><span class="n">gbx</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">uin</span><span class="p">[</span><span class="n">mfi</span><span class="p">]);</span>

    <span class="c1">// Then do more work with q as input and uout[mfi] as output.</span>
    <span class="c1">// The output region is vbx.</span>
    <span class="n">f2</span><span class="p">(</span><span class="n">vbx</span><span class="p">,</span> <span class="n">uout</span><span class="p">[</span><span class="n">mfi</span><span class="p">],</span> <span class="n">q</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>There are several issues in migrating this code to GPUs that need to
be addressed.  First, functions <code class="docutils literal notranslate"><span class="pre">f1</span></code> and <code class="docutils literal notranslate"><span class="pre">f2</span></code> have different
work regions (<code class="docutils literal notranslate"><span class="pre">vbx</span></code> and <code class="docutils literal notranslate"><span class="pre">gbx</span></code>, respectively) and there are data
dependencies between the two (<code class="docutils literal notranslate"><span class="pre">q</span></code>). This makes it difficult to put
them into a single GPU kernel, so two separate kernels will be
launched, one for each function.</p>
<p>As we have discussed in Section <a class="reference internal" href="#sec-gpu-classes"><span class="std std-ref">GPU Safe Classes and Functions</span></a>, all
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code>es in the two <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span></code>s, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">uin</span></span></code>
and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">uout</span></span></code> are in unified memory and avaiable on the GPUs.
But <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span> <span class="name"><span class="pre">q</span></span></code> is in host memory.  Creating <code class="docutils literal notranslate"><span class="pre">q</span></code> as a
managed object using the overloaded <code class="code cpp c++ docutils literal notranslate"><span class="keyword"><span class="pre">new</span></span></code> operator:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">FArrayBox</span><span class="o">*</span> <span class="n">q</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FArrayBox</span><span class="p">;</span>
</pre></div>
</div>
<p>does not solve the problem completely because GPU kernel calls are
asynchronous from CPU’s point of view.  This creates a race
condition: GPU kernels in different iterations of <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code>
will compete for access to <code class="docutils literal notranslate"><span class="pre">q</span></code>.  One possible failure is a
segfault when <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">resize</span></span></code> changes the size of the <code class="docutils literal notranslate"><span class="pre">q</span></code> object
when the previous iteration of the loop is still using an old size.</p>
<p>Moving the line into the body of <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loop will make <code class="docutils literal notranslate"><span class="pre">q</span></code>
a variable local to each iteration, but it has a new issue.  When
do we delete <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">q</span></span></code>?  To the CPU, the resource of <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">q</span></span></code>
should be freed at the end of the scope, otherwise there will be
a memory leak.  But at the end of the CPU scope, GPU kernels might
still need it.</p>
<p>One way to fix this is put the temporary <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code> objects in a
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span></code> defined outside the loop.  This creates a separate
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">FArrayBox</span></span></code> for each loop iteration, eliminating the race
condition.  Another way is to use <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">AsyncFab</span></span></code> designed for
this kind of situation.  The code below shows how <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">AsyncFab</span></span></code>
is used and how this MFIter loop can be rewritten for GPUs.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">MFIter</span> <span class="n">mfi</span><span class="p">(</span><span class="n">uin</span><span class="p">);</span> <span class="n">mfi</span><span class="p">.</span><span class="n">isValid</span><span class="p">();</span> <span class="o">++</span><span class="n">mfi</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="n">Box</span><span class="o">&amp;</span> <span class="n">vbx</span> <span class="o">=</span> <span class="n">mfi</span><span class="p">.</span><span class="n">validbox</span><span class="p">();</span>              <span class="c1">// f2 work domain</span>
    <span class="k">const</span> <span class="n">Box</span><span class="o">&amp;</span> <span class="n">gbx</span> <span class="o">=</span> <span class="n">amrex</span><span class="o">::</span><span class="n">grow</span><span class="p">(</span><span class="n">vbx</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>          <span class="c1">// f1 work domain</span>
    <span class="n">Gpu</span><span class="o">::</span><span class="n">AsyncFab</span> <span class="n">q</span><span class="p">(</span><span class="n">gbx</span><span class="p">);</span>                         <span class="c1">// Local, GPU managed FArrayBox</span>
    <span class="n">FArrayBox</span> <span class="k">const</span><span class="o">*</span> <span class="n">uinfab</span>  <span class="o">=</span> <span class="n">uin</span><span class="p">.</span><span class="n">fabPtr</span><span class="p">();</span>      <span class="c1">// Managed GPU capturable</span>
    <span class="n">FArrayBox</span>      <span class="o">*</span> <span class="n">uoutfab</span> <span class="o">=</span> <span class="n">uout</span><span class="p">.</span><span class="n">fabPtr</span><span class="p">();</span>     <span class="c1">//   pointers to MultiFab&#39;s FABs.</span>

    <span class="n">AMREX_LAUNCH_DEVICE_LAMBDA</span> <span class="p">(</span> <span class="n">gbx</span><span class="p">,</span> <span class="n">tbx</span><span class="p">,</span>        <span class="c1">// f1 GPU launch</span>
    <span class="p">{</span>
        <span class="n">f1</span><span class="p">(</span><span class="n">tbx</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">fab</span><span class="p">(),</span> <span class="o">*</span><span class="n">uinfab</span><span class="p">);</span>
    <span class="p">};</span>

    <span class="n">AMrEX_LAMBDA_DEVICE_LAMBDA</span> <span class="p">(</span> <span class="n">vbx</span><span class="p">,</span> <span class="n">tbx</span><span class="p">,</span>        <span class="c1">// f2 GPU launch</span>
    <span class="p">{</span>
        <span class="n">f2</span><span class="p">(</span><span class="n">tbx</span><span class="p">,</span> <span class="o">*</span><span class="n">uoutfab</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">fab</span><span class="p">());</span>
    <span class="p">});</span>
<span class="p">}</span>                                                 <span class="c1">// Implicit GPU barrier after</span>
                                                  <span class="c1">//   all iters are launched.</span>
</pre></div>
</div>
</div>
<div class="section" id="assertions-error-checking-and-synchronization">
<span id="sec-gpu-assertion"></span><h1>Assertions, Error Checking and Synchronization<a class="headerlink" href="#assertions-error-checking-and-synchronization" title="Permalink to this headline">¶</a></h1>
<p>To help debugging, we often use <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">Assert</span></span></code> and
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">Abort</span></span></code>.  These functions are GPU safe and can be used in
GPU kernels.  However, implementing these functions requires additional
GPU registers, which will reduce overall performance.  Therefore, it
is preferred to implement such calls in debug mode only by wraping the
calls using <code class="docutils literal notranslate"><span class="pre">#ifdef</span> <span class="pre">AMREX_DEBUG</span></code>.</p>
<p>In CPU code, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">AMREX_GPU_ERROR_CHECK</span></span><span class="punctuation"><span class="pre">()</span></span></code> can be called
to check the health of previous GPU launches.  This call
looks up the return message from the most recently completed GPU
launch and aborts if it was not successful. Many kernel
launch macros as well as the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> destructor include a call
to <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">AMREX_GPU_ERROR_CHECK</span></span><span class="punctuation"><span class="pre">()</span></span></code>. This prevents additional launches
from being called if a previous launch caused an error and ensures
all GPU launches within an <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loop completed successfully
before continuing work.</p>
<p>However, due to asynchronicity, determining the source of the error
can be difficult.  Even if GPU kernels launched earlier in the code
result in a CUDA error, the error may not be output at a nearby call to
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">AMREX_GPU_ERROR_CHECK</span></span><span class="punctuation"><span class="pre">()</span></span></code> by the CPU.  When tracking down a CUDA
launch error, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">Device</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">synchronize</span></span><span class="punctuation"><span class="pre">()</span></span></code> and
<code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Gpu</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">Device</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">streamSynchronize</span></span><span class="punctuation"><span class="pre">()</span></span></code> can be used to synchronize
the device or the CUDA stream, respectively, and track down the specific
launch that causes the error.</p>
</div>
<div class="section" id="particle-support">
<h1>Particle Support<a class="headerlink" href="#particle-support" title="Permalink to this headline">¶</a></h1>
<p id="sec-gpu-particle">AMReX’s GPU particle support relies on Thrust, a parallel algorithms library maintained by
Nvidia. Thrust provides a GPU-capable vector container that is otherwise similar to the one
in the C++ Standard Template Library, along with associated sorting, searching, and prefix
summing operations. Combined with Cuda’s unified memory, Thrust forms the basis of AMReX’s
GPU support for particles.</p>
<p>When compiled with <code class="docutils literal notranslate"><span class="pre">USE_CUDA=TRUE</span></code>, AMReX places all its particle data in instances of
<code class="docutils literal notranslate"><span class="pre">thrust::device_vector</span></code> that have been configured using a custom memory allocator using
<code class="docutils literal notranslate"><span class="pre">cudaMallocManaged</span></code>. This means that the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">dataPtr</span></span></code> associated with particle data
is managed and can be passed into GPU kernels, similar to the way it would be passed into
a Fortran subroutine in typical AMReX CPU code. As with the mesh data, these kernels can
be launched with a variety of approaches, including Cuda C / Fortran and OpenACC. An example
Fortran particle subroutine offloaded via OpenACC might look like the following:</p>
<div class="highlight-fortran notranslate"><div class="highlight"><pre><span></span><span class="k">subroutine </span><span class="n">push_position_boris</span><span class="p">(</span><span class="n">np</span><span class="p">,</span> <span class="n">structs</span><span class="p">,</span> <span class="n">uxp</span><span class="p">,</span> <span class="n">uyp</span><span class="p">,</span> <span class="n">uzp</span><span class="p">,</span> <span class="n">gaminv</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>

<span class="k">use </span><span class="n">em_particle_module</span><span class="p">,</span> <span class="n">only</span> <span class="p">:</span> <span class="n">particle_t</span>
<span class="k">use </span><span class="n">amrex_fort_module</span><span class="p">,</span> <span class="n">only</span> <span class="p">:</span> <span class="n">amrex_real</span>
<span class="k">implicit none</span>

<span class="kt">integer</span><span class="p">,</span>          <span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">),</span> <span class="k">value</span>  <span class="kd">::</span> <span class="n">np</span>
<span class="k">type</span><span class="p">(</span><span class="n">particle_t</span><span class="p">),</span> <span class="k">intent</span><span class="p">(</span><span class="n">inout</span><span class="p">)</span>      <span class="kd">::</span> <span class="n">structs</span><span class="p">(</span><span class="n">np</span><span class="p">)</span>
<span class="kt">real</span><span class="p">(</span><span class="n">amrex_real</span><span class="p">),</span> <span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span>         <span class="kd">::</span> <span class="n">uxp</span><span class="p">(</span><span class="n">np</span><span class="p">),</span> <span class="n">uyp</span><span class="p">(</span><span class="n">np</span><span class="p">),</span> <span class="n">uzp</span><span class="p">(</span><span class="n">np</span><span class="p">),</span> <span class="n">gaminv</span><span class="p">(</span><span class="n">np</span><span class="p">)</span>
<span class="kt">real</span><span class="p">(</span><span class="n">amrex_real</span><span class="p">),</span> <span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">),</span> <span class="k">value</span>  <span class="kd">::</span> <span class="n">dt</span>

<span class="kt">integer</span>                              <span class="kd">::</span> <span class="n">ip</span>

<span class="c">!$acc parallel deviceptr(structs, uxp, uyp, uzp, gaminv)</span>
<span class="c">!$acc loop gang vector</span>
<span class="k">do </span><span class="n">ip</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span>
    <span class="n">structs</span><span class="p">(</span><span class="n">ip</span><span class="p">)%</span><span class="n">pos</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="n">structs</span><span class="p">(</span><span class="n">ip</span><span class="p">)%</span><span class="n">pos</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">uxp</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span><span class="o">*</span><span class="n">gaminv</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span><span class="o">*</span><span class="n">dt</span>
    <span class="n">structs</span><span class="p">(</span><span class="n">ip</span><span class="p">)%</span><span class="n">pos</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">=</span> <span class="n">structs</span><span class="p">(</span><span class="n">ip</span><span class="p">)%</span><span class="n">pos</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">uxp</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span><span class="o">*</span><span class="n">gaminv</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span><span class="o">*</span><span class="n">dt</span>
    <span class="n">structs</span><span class="p">(</span><span class="n">ip</span><span class="p">)%</span><span class="n">pos</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">=</span> <span class="n">structs</span><span class="p">(</span><span class="n">ip</span><span class="p">)%</span><span class="n">pos</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="n">uxp</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span><span class="o">*</span><span class="n">gaminv</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span><span class="o">*</span><span class="n">dt</span>
<span class="k">end do</span>
<span class="c">!$acc end loop</span>
<span class="c">!$acc end parallel</span>

<span class="k">end subroutine </span><span class="n">push_position_boris</span>
</pre></div>
</div>
<p>Note the use of the <code class="code fortran docutils literal notranslate"><span class="comment"><span class="pre">!$acc</span> <span class="pre">parallel</span> <span class="pre">deviceptr</span></span></code> clause to specify which data has been placed
in managed memory. This instructs OpenACC to treat those variables as if they already live on
the device, bypassing the usual copies. For a complete example of a particle code that has been ported
to GPUs using OpenACC, please see <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Tutorials</span></span><span class="operator"><span class="pre">/</span></span><span class="name"><span class="pre">Particles</span></span><span class="operator"><span class="pre">/</span></span><span class="name"><span class="pre">ElectromagneticPIC</span></span></code>.</p>
<p>For portability, we have provided a set of Vector classes that wrap around the Thrust and
STL vectors. When <code class="docutils literal notranslate"><span class="pre">USE_CUDA</span> <span class="pre">=</span> <span class="pre">FALSE</span></code>, these classes reduce to the normal <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">amrex</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">Vector</span></span></code>.
When <code class="docutils literal notranslate"><span class="pre">USE_CUDA</span> <span class="pre">=</span> <span class="pre">TRUE</span></code>, they have different meanings. <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Cuda</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">HostVector</span></span></code> is a wrapper
around <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">thrust</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">host_vector</span></span></code>. <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Cuda</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">DeviceVector</span></span></code> is a wrapper around <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">thrust</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">device_vector</span></span></code>,
while <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Cuda</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">ManagedDeviceVector</span></span></code> is a <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">thrust</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">device_vector</span></span></code> that lives in managed memory.
These classes are useful when there are certain stages of an algorithm that will always
execute on either the host or the device. For example, the following code generates particles on
the CPU and copies them over to the GPU in one batch per tile:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="p">(</span><span class="n">MFIter</span> <span class="n">mfi</span> <span class="o">=</span> <span class="n">MakeMFIter</span><span class="p">(</span><span class="n">lev</span><span class="p">);</span> <span class="n">mfi</span><span class="p">.</span><span class="n">isValid</span><span class="p">();</span> <span class="o">++</span><span class="n">mfi</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="n">Box</span><span class="o">&amp;</span> <span class="n">tile_box</span>  <span class="o">=</span> <span class="n">mfi</span><span class="p">.</span><span class="n">tilebox</span><span class="p">();</span>
    <span class="n">Cuda</span><span class="o">::</span><span class="n">HostVector</span><span class="o">&lt;</span><span class="n">ParticleType</span><span class="o">&gt;</span> <span class="n">host_particles</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">IntVect</span> <span class="n">iv</span> <span class="o">=</span> <span class="n">tile_box</span><span class="p">.</span><span class="n">smallEnd</span><span class="p">();</span> <span class="n">iv</span> <span class="o">&lt;=</span> <span class="n">tile_box</span><span class="p">.</span><span class="n">bigEnd</span><span class="p">();</span> <span class="n">tile_box</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">iv</span><span class="p">))</span>
    <span class="p">{</span>
        <span class="o">&lt;</span> <span class="n">generate</span> <span class="n">some</span> <span class="n">particles</span><span class="p">...</span> <span class="o">&gt;</span>
    <span class="p">}</span>

    <span class="k">auto</span><span class="o">&amp;</span> <span class="n">particles</span> <span class="o">=</span> <span class="n">GetParticles</span><span class="p">(</span><span class="n">lev</span><span class="p">);</span>
    <span class="k">auto</span><span class="o">&amp;</span> <span class="n">particle_tile</span> <span class="o">=</span> <span class="n">particles</span><span class="p">[</span><span class="n">std</span><span class="o">::</span><span class="n">make_pair</span><span class="p">(</span><span class="n">mfi</span><span class="p">.</span><span class="n">index</span><span class="p">(),</span> <span class="n">mfi</span><span class="p">.</span><span class="n">LocalTileIndex</span><span class="p">())];</span>
    <span class="k">auto</span> <span class="n">old_size</span> <span class="o">=</span> <span class="n">particle_tile</span><span class="p">.</span><span class="n">GetArrayOfStructs</span><span class="p">().</span><span class="n">size</span><span class="p">();</span>
    <span class="k">auto</span> <span class="n">new_size</span> <span class="o">=</span> <span class="n">old_size</span> <span class="o">+</span> <span class="n">host_particles</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
    <span class="n">particle_tile</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">new_size</span><span class="p">);</span>

    <span class="n">Cuda</span><span class="o">::</span><span class="n">thrust_copy</span><span class="p">(</span><span class="n">host_particles</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
                      <span class="n">host_particles</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span>
                      <span class="n">particle_tile</span><span class="p">.</span><span class="n">GetArrayOfStructs</span><span class="p">().</span><span class="n">begin</span><span class="p">()</span> <span class="o">+</span> <span class="n">old_size</span><span class="p">);</span>
 <span class="p">}</span>
</pre></div>
</div>
<p>The following example shows how to use <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Cuda</span></span><span class="operator"><span class="pre">::</span></span><span class="name"><span class="pre">DeviceVector</span></span></code>. Specifically, this code creates
temporary device vectors for the particle x, y, and z positions, and then copies from an Array-of-Structs
to a Struct-of-Arrays representation, all without copying any particle data off the GPU:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Cuda</span><span class="o">::</span><span class="n">DeviceVector</span><span class="o">&lt;</span><span class="n">Real</span><span class="o">&gt;</span> <span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">,</span> <span class="n">zp</span><span class="p">;</span>

<span class="k">for</span> <span class="p">(</span><span class="n">WarpXParIter</span> <span class="n">pti</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">,</span> <span class="n">lev</span><span class="p">);</span> <span class="n">pti</span><span class="p">.</span><span class="n">isValid</span><span class="p">();</span> <span class="o">++</span><span class="n">pti</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">pti</span><span class="p">.</span><span class="n">GetPosition</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">,</span> <span class="n">zp</span><span class="p">);</span>

    <span class="o">&lt;</span> <span class="n">use</span> <span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">,</span> <span class="n">zp</span><span class="p">...</span> <span class="o">&gt;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Note that the above code will cause problems if multiple streams are used to launch kernels inside the
particle iterator loop. This is because the temporary variables <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">xp</span></span></code>, <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">yp</span></span></code>, and <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">zp</span></span></code> are
shared between different iterations. However, if all the kernel launches happen on the default stream,
so that the kernels are guaranteed to complete in order, then the above approach will give the
expected results.</p>
<p>Finally, AMReX’s <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">Redistribute</span></span><span class="punctuation"><span class="pre">()</span></span></code>, which moves particles back to the proper grids after their positions
have changed, has been ported to work on the GPU as well. It cannot be called from device code,
but it can be called on particles that reside on the device and it won’t trigger any unified
memory traffic. As with <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MultiFab</span></span></code> data, the MPI portion of the particle redistribute is set
up to take advantange of the Cuda-aware MPI implementations available on platforms such as
ORNL’s Summit and Summit-dev.</p>
</div>
<div class="section" id="profiling-with-gpus">
<h1>Profiling with GPUs<a class="headerlink" href="#profiling-with-gpus" title="Permalink to this headline">¶</a></h1>
<p id="sec-gpu-profiling">When profiling for GPUs, AMReX recommends <code class="docutils literal notranslate"><span class="pre">nvprof</span></code>, NVIDIA’s visual
profiler.  <code class="docutils literal notranslate"><span class="pre">nvprof</span></code> returns data on how long each kernel launch lasted on
the GPU, the number of threads and registers used, the occupancy of the GPU
and recommendations for improving the code.  For more information on how to
use <code class="docutils literal notranslate"><span class="pre">nvprof</span></code>, see NVIDIA’s User’s Guide as well as the help webpages of
your favorite supercomputing facility that uses NVIDIA GPUs.</p>
<p>AMReX’s internal profilers currently cannot hook into profiling information
on the GPU and an efficient way to time and retrieve that information is
being explored. In the meantime, AMReX’s timers can be used to report some
generic timers that are useful in categorizing an application.</p>
<p>Due to the asynchonous launching of GPU kernels, any AMReX timers inside of
asynchronous regions or inside GPU kernels will not measure useful
information.  However, since the <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> synchronizes when being
destroyed, any timer wrapped around an <code class="code cpp c++ docutils literal notranslate"><span class="name"><span class="pre">MFIter</span></span></code> loop will yield a
consistent timing of the entire set of GPU launches contained within. For
example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">BL_PROFILE_VAR</span><span class="p">(</span><span class="s">&quot;MFIter: Init&quot;</span><span class="p">,</span> <span class="n">mfinit</span><span class="p">);</span>     <span class="c1">// Profiling start</span>
<span class="k">for</span> <span class="p">(</span><span class="n">MFIter</span> <span class="n">mfi</span><span class="p">(</span><span class="n">phi_new</span><span class="p">);</span> <span class="n">mfi</span><span class="p">.</span><span class="n">isValid</span><span class="p">();</span> <span class="o">++</span><span class="n">mfi</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="n">Box</span><span class="o">&amp;</span> <span class="n">vbx</span> <span class="o">=</span> <span class="n">mfi</span><span class="p">.</span><span class="n">validbox</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">GeometryData</span><span class="o">&amp;</span> <span class="n">geomdata</span> <span class="o">=</span> <span class="n">geom</span><span class="p">.</span><span class="n">data</span><span class="p">();</span>
    <span class="n">FArrayBox</span><span class="o">*</span> <span class="n">phiNew</span> <span class="o">=</span> <span class="n">phi_new</span><span class="p">.</span><span class="n">fabPtr</span><span class="p">(</span><span class="n">mfi</span><span class="p">);</span>

    <span class="n">AMREX_LAUNCH_DEVICE_LAMBDA</span><span class="p">(</span><span class="n">vbx</span><span class="p">,</span> <span class="n">tbx</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="n">init_phi</span><span class="p">(</span><span class="n">BL_TO_FORTRAN_BOX</span><span class="p">(</span><span class="n">tbx</span><span class="p">),</span>
                 <span class="n">BL_TO_FORTRAN_ANYD</span><span class="p">(</span><span class="o">*</span><span class="n">phiNew</span><span class="p">),</span>
                 <span class="n">geomdata</span><span class="p">.</span><span class="n">CellSize</span><span class="p">(),</span> <span class="n">geomdata</span><span class="p">.</span><span class="n">ProbLo</span><span class="p">(),</span> <span class="n">geomdata</span><span class="p">.</span><span class="n">ProbHi</span><span class="p">());</span>
    <span class="p">});</span>

<span class="p">}</span>
<span class="n">BL_PROFILE_STOP</span><span class="p">(</span><span class="n">mfinit</span><span class="p">);</span>                    <span class="c1">// Profiling stop</span>
</pre></div>
</div>
<p>For now, this is the best way to profile GPU codes using <code class="docutils literal notranslate"><span class="pre">TinyProfiler</span></code>.
If you require further profiling detail, use <code class="docutils literal notranslate"><span class="pre">nvprof</span></code>.</p>
</div>
<div class="section" id="performance-tips">
<h1>Performance Tips<a class="headerlink" href="#performance-tips" title="Permalink to this headline">¶</a></h1>
<p id="sec-gpu-performance">Here are some helpful performance tips to keep in mind when working with
AMReX for GPUs:</p>
<ul class="simple">
<li></li>
<li></li>
<li></li>
</ul>
</div>
<div class="section" id="limitations">
<h1>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline">¶</a></h1>
<p id="sec-gpu-limits">GPU support in AMReX is still under development.  There are some known
limitations:</p>
<ul class="simple">
<li>By default, AMReX assumes the MPI library used is GPU aware.  The
communication buffers given to MPI functions are allocated in device
memory.</li>
<li>OpenMP is currently not compatible with building AMReX with CUDA.
<code class="docutils literal notranslate"><span class="pre">USE_CUDA=TRUE</span></code> and <code class="docutils literal notranslate"><span class="pre">USE_OMP=TRUE</span></code> will fail to compile.</li>
<li>CMake is not yet supported for building AMReX GPU support.</li>
<li>Many multi-level functions in AMReX have not been ported to GPUs.</li>
<li>Linear solvers have not been ported to GPUs.</li>
<li>Embedded boundary capability has not been ported to GPUs.</li>
<li>The Fortran interface of AMReX does not currently have GPU support.</li>
</ul>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Visualization_Chapter.html" class="btn btn-neutral float-right" title="Visualization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="GPU_Chapter.html" class="btn btn-neutral" title="GPU" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2018, AMReX Team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>